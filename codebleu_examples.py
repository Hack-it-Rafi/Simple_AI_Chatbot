#!/usr/bin/env python3
"""
Example script demonstrating CodeBLEU evaluation for the AI chatbot.
This script shows how to evaluate code generation quality using various methods.
"""

import sys
import os
sys.path.append(os.path.dirname(__file__))

from codebleu_evaluator import CodeBLEUEvaluator, ChatbotCodeEvaluator


def example_basic_evaluation():
    """Example of basic CodeBLEU evaluation with two code snippets."""
    print("=== Basic CodeBLEU Evaluation Example ===\n")
    
    # Initialize evaluator
    evaluator = CodeBLEUEvaluator()
    
    # Example generated code (what your chatbot might generate)
    generated_code = """
def fibonacci(n):
    if n <= 1:
        return n
    return fibonacci(n-1) + fibonacci(n-2)

def main():
    print(fibonacci(10))
"""
    
    # Reference implementation
    reference_code = """
def fibonacci(n):
    if n <= 1:
        return n
    else:
        return fibonacci(n-1) + fibonacci(n-2)

if __name__ == "__main__":
    result = fibonacci(10)
    print(result)
"""
    
    # Evaluate
    results = evaluator.evaluate(generated_code, reference_code)
    
    print(f"Generated Code:\n{generated_code}")
    print(f"\nReference Code:\n{reference_code}")
    print(f"\nEvaluation Results:")
    print(f"CodeBLEU Score: {results['codebleu']:.4f}")
    print(f"BLEU Score: {results['bleu']:.4f}")
    print(f"Weighted BLEU: {results['weighted_bleu']:.4f}")
    print(f"AST Matching: {results['ast_match']:.4f}")
    print(f"Control Flow: {results['control_flow']:.4f}")
    print(f"Detected Language: {results['language']}")
    print()


def example_chatbot_evaluation():
    """Example of evaluating code generated by your chatbot server."""
    print("=== Chatbot Integration Example ===\n")
    
    # Initialize chatbot evaluator
    chatbot_eval = ChatbotCodeEvaluator(server_url="http://localhost:3000")
    
    # Define a prompt and reference implementation
    prompt = "Write a Python function to calculate the factorial of a number"
    
    reference_code = """
def factorial(n):
    if n <= 1:
        return 1
    return n * factorial(n-1)
"""
    
    print(f"Prompt: {prompt}")
    print(f"Reference Implementation:\n{reference_code}")
    
    # Evaluate (this would call your running chatbot server)
    try:
        results = chatbot_eval.evaluate_prompt_with_reference(prompt, reference_code)
        
        if results:
            print(f"\nChatbot Evaluation Results:")
            print(f"CodeBLEU Score: {results['codebleu']:.4f}")
            print(f"BLEU Score: {results['bleu']:.4f}")
            print(f"Weighted BLEU: {results['weighted_bleu']:.4f}")
            print(f"AST Matching: {results['ast_match']:.4f}")
            print(f"Control Flow: {results['control_flow']:.4f}")
        else:
            print("Failed to evaluate chatbot response (server may not be running)")
    except Exception as e:
        print(f"Error during chatbot evaluation: {e}")
        print("Make sure your server is running on http://localhost:3000")
    print()


def example_javascript_evaluation():
    """Example of evaluating JavaScript code generation."""
    print("=== JavaScript Code Evaluation Example ===\n")
    
    evaluator = CodeBLEUEvaluator()
    
    # JavaScript code examples
    generated_js = """
function quickSort(arr) {
    if (arr.length <= 1) {
        return arr;
    }
    const pivot = arr[0];
    const left = [];
    const right = [];
    for (let i = 1; i < arr.length; i++) {
        if (arr[i] < pivot) {
            left.push(arr[i]);
        } else {
            right.push(arr[i]);
        }
    }
    return quickSort(left).concat([pivot]).concat(quickSort(right));
}
"""
    
    reference_js = """
function quickSort(array) {
    if (array.length <= 1) {
        return array;
    }
    const pivot = array[0];
    const left = [];
    const right = [];
    for (let i = 1; i < array.length; i++) {
        if (array[i] < pivot) {
            left.push(array[i]);
        } else {
            right.push(array[i]);
        }
    }
    return [...quickSort(left), pivot, ...quickSort(right)];
}
"""
    
    results = evaluator.evaluate(generated_js, reference_js, language='javascript')
    
    print("Generated JavaScript Code:")
    print(generated_js)
    print("\nReference JavaScript Code:")
    print(reference_js)
    print(f"\nEvaluation Results:")
    print(f"CodeBLEU Score: {results['codebleu']:.4f}")
    print(f"BLEU Score: {results['bleu']:.4f}")
    print(f"Weighted BLEU: {results['weighted_bleu']:.4f}")
    print(f"AST Matching: {results['ast_match']:.4f}")
    print(f"Control Flow: {results['control_flow']:.4f}")
    print()


def example_custom_weights():
    """Example of using custom weights for different components."""
    print("=== Custom Weights Example ===\n")
    
    # Create evaluator with custom weights (emphasizing AST matching)
    evaluator = CodeBLEUEvaluator(alpha=0.1, beta=0.1, gamma=0.6, delta=0.2)
    
    code1 = """
def process_data(data):
    result = []
    for item in data:
        if item > 0:
            result.append(item * 2)
    return result
"""
    
    code2 = """
def process_data(data):
    result = []
    for element in data:
        if element > 0:
            result.append(element * 2)
    return result
"""
    
    results = evaluator.evaluate(code1, code2)
    
    print(f"Evaluator weights: α=0.1, β=0.1, γ=0.6, δ=0.2")
    print(f"(Emphasizing AST structure over token similarity)")
    print(f"\nCodeBLEU Score: {results['codebleu']:.4f}")
    print(f"AST Matching: {results['ast_match']:.4f}")
    print(f"BLEU Score: {results['bleu']:.4f}")
    print()


if __name__ == "__main__":
    print("CodeBLEU Evaluator Examples")
    print("=" * 50)
    print()
    
    # Run examples
    example_basic_evaluation()
    example_javascript_evaluation()
    example_custom_weights()
    example_chatbot_evaluation()
    
    print("=" * 50)
    print("Examples completed!")
    print("\nTo use the evaluator with command line:")
    print("python codebleu_evaluator.py --candidate generated.py --reference reference.py")
    print("python codebleu_evaluator.py --prompt \"write a sort function\" --reference reference.py --server-url http://localhost:3000")